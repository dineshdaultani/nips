{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Importing Necessary Packages\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## converting Ascii to UTF-8\n",
    "## For writing text conveniently\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "print sys.getdefaultencoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Checking whether encoding is changed or not\n",
    "import sys\n",
    "print sys.getdefaultencoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://papers.nips.cc/book/advances-in-neural-information-processing-systems-28-2015', 'http://papers.nips.cc/book/advances-in-neural-information-processing-systems-27-2014', 'http://papers.nips.cc/book/advances-in-neural-information-processing-systems-26-2013', 'http://papers.nips.cc/book/advances-in-neural-information-processing-systems-25-2012', 'http://papers.nips.cc/book/advances-in-neural-information-processing-systems-24-2011', 'http://papers.nips.cc/book/advances-in-neural-information-processing-systems-23-2010', 'http://papers.nips.cc/book/advances-in-neural-information-processing-systems-22-2009', 'http://papers.nips.cc/book/advances-in-neural-information-processing-systems-21-2008', 'http://papers.nips.cc/book/advances-in-neural-information-processing-systems-20-2007', 'http://papers.nips.cc/book/advances-in-neural-information-processing-systems-19-2006', 'http://papers.nips.cc/book/advances-in-neural-information-processing-systems-18-2005', 'http://papers.nips.cc/book/advances-in-neural-information-processing-systems-17-2004', 'http://papers.nips.cc/book/advances-in-neural-information-processing-systems-16-2003', 'http://papers.nips.cc/book/advances-in-neural-information-processing-systems-15-2002', 'http://papers.nips.cc/book/advances-in-neural-information-processing-systems-14-2001', 'http://papers.nips.cc/book/advances-in-neural-information-processing-systems-13-2000', 'http://papers.nips.cc/book/advances-in-neural-information-processing-systems-12-1999', 'http://papers.nips.cc/book/advances-in-neural-information-processing-systems-11-1998', 'http://papers.nips.cc/book/advances-in-neural-information-processing-systems-10-1997', 'http://papers.nips.cc/book/advances-in-neural-information-processing-systems-9-1996', 'http://papers.nips.cc/book/advances-in-neural-information-processing-systems-8-1995', 'http://papers.nips.cc/book/advances-in-neural-information-processing-systems-7-1994', 'http://papers.nips.cc/book/advances-in-neural-information-processing-systems-6-1993', 'http://papers.nips.cc/book/advances-in-neural-information-processing-systems-5-1992', 'http://papers.nips.cc/book/advances-in-neural-information-processing-systems-4-1991', 'http://papers.nips.cc/book/advances-in-neural-information-processing-systems-3-1990', 'http://papers.nips.cc/book/advances-in-neural-information-processing-systems-2-1989', 'http://papers.nips.cc/book/advances-in-neural-information-processing-systems-1-1988', 'https://papers.nips.cc/book/neural-information-processing-systems-1987']\n"
     ]
    }
   ],
   "source": [
    "## Generating URL's of each years paper homepage\n",
    "base_url  = \"http://papers.nips.cc\"\n",
    "index_conf_year = 2015\n",
    "index_conf_number = 28\n",
    "index_url_array = []\n",
    "for i in range(28):\n",
    "    a = 'http://papers.nips.cc/book/advances-in-neural-information-processing-systems-%i-%i' %(index_conf_number,index_conf_year)\n",
    "    index_url_array.append(a)\n",
    "    index_conf_year -= 1\n",
    "    index_conf_number -= 1\n",
    "## Appending last year URL explicitly due to change in Format\n",
    "index_url_array.append(\"https://papers.nips.cc/book/neural-information-processing-systems-1987\")    \n",
    "print index_url_array\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function for converting pdf to text via 'pdftotext' program\n",
    "## Explicitly need to install 'pdftotext' software on the operating system\n",
    "def text_from_pdf(pdf_path, temp_path):\n",
    "    if os.path.exists(temp_path):\n",
    "        os.remove(temp_path)\n",
    "    subprocess.call([\"pdftotext\", pdf_path, temp_path])\n",
    "    f = open(temp_path)\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    os.remove(temp_path)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Iteration loop for each year's data from 2015\n",
    "index_conf_year = 2015\n",
    "for i,url in enumerate(index_url_array):\n",
    "    index_url = index_url_array[i]\n",
    "    r = requests.get(index_url)\n",
    "    ## BeautifulSoup package converts the HTML page into JSON format for convenient readability\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    paper_links = [link for link in soup.find_all('a') if link[\"href\"][:7]==\"/paper/\"]\n",
    "    ## Total Paper found every year\n",
    "    print(\"%d Papers Found\" % len(paper_links))\n",
    "    ## Initializing all lists()\n",
    "    nips_authors = list()\n",
    "    papers = list()\n",
    "    paper_authors = list()\n",
    "    authors_all = list()\n",
    "    email_author =  list()\n",
    "    email_id = list()\n",
    "    emails = []\n",
    "    tempEmail = []\n",
    "    event_type = []\n",
    "    \n",
    "    temp_path = os.path.join(\"output\", \"temp.txt\")\n",
    "    ## Iteration Loop for every year's each pdf or paper\n",
    "    for link in paper_links:\n",
    "        paper_title = link.contents[0]\n",
    "        info_link = base_url + link[\"href\"]\n",
    "        pdf_link = info_link + \".pdf\"\n",
    "        pdf_name = link[\"href\"][7:] + \".pdf\"\n",
    "        paper_id = re.findall(r\"^(\\d+)-\", pdf_name)[0]\n",
    "        pdf = requests.get(pdf_link)\n",
    "        folder_name = '%i' %index_conf_year\n",
    "        ## Saving paper pdf to specified folders\n",
    "        pdf_path = os.path.join(\"output\", \"pdfs\", folder_name, pdf_name)\n",
    "        pdf_file = open(pdf_path, \"wb\")\n",
    "        pdf_file.write(pdf.content)\n",
    "        pdf_file.close()\n",
    "        ## getting the whole webpage data in JSON format from Beautiful Soup.\n",
    "        paper_soup = BeautifulSoup(requests.get(info_link).content, \"lxml\")\n",
    "        ## Getting abstract from BeautifulSoup.\n",
    "        abstract = paper_soup.find('p', attrs={\"class\": \"abstract\"}).contents[0]\n",
    "        ## Getting authors from BeautifulSoup.\n",
    "        authors = [(re.findall(r\"-(\\d+)$\", author.contents[0][\"href\"])[0],\n",
    "                    author.contents[0].contents[0])\n",
    "                   for author in paper_soup.find_all('li', attrs={\"class\": \"author\"})]\n",
    "        \n",
    "        ## Getting Event_type from BeautifulSoup.\n",
    "        event_types = [h.contents[0][23:] for h in paper_soup.find_all('h3') if h.contents[0][:22]==\"Conference Event Type:\"]\n",
    "        ## sometimes event_type is empty, hence appending 'poster' explicitly if the event is empty\n",
    "        if not event_types:\n",
    "                event_types.append(\"Poster\")\n",
    "        if len(event_types) != 1:\n",
    "            print([h.contents for h in paper_soup.find_all('h3')])\n",
    "            raise Exception(\"Bad Event Data\")    \n",
    "        event_type = event_types[0]\n",
    "        \n",
    "        paper_text = text_from_pdf(pdf_path, temp_path)\n",
    "        print(paper_title)\n",
    "        \n",
    "        ## Generating Email address from pdf text\n",
    "        emails = []  \n",
    "        # Regular expression for extraction of email ID.\n",
    "        emails = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', paper_text)\n",
    "        if not emails:\n",
    "            rx = r'\\{(?P<individual>[^{}]+)\\}@(?P<domain>\\S+)'\n",
    "            for match in re.finditer(rx, paper_text):\n",
    "                email_author = [x.strip() for x in (match.group('individual')).split(',')]\n",
    "            for email in email_author:\n",
    "                tempEmail = email + '@' +match.group('domain')\n",
    "                emails.append(tempEmail)\n",
    "\n",
    "        ## appending data to nips_author and paper_author lists\n",
    "        for j,author in enumerate(authors):\n",
    "            \n",
    "            try:\n",
    "                nips_authors.append([authors[j][0], paper_id ,authors[j][1], emails[j], index_conf_year])\n",
    "                print nips_authors[j]\n",
    "                paper_authors.append([len(paper_authors)+1, paper_id, author[0]])\n",
    "            except:\n",
    "                pass\n",
    "        ## appending data to papers lists\n",
    "        papers.append([paper_id, paper_title, event_type, pdf_name, abstract, paper_text])\n",
    "    ## decrementing year\n",
    "    index_conf_year -= 1\n",
    "    ## writing the appended lists to csv files with append true option.\n",
    "    pd.DataFrame(nips_authors, columns=[\"Id\",\"Name\",\"Email\",\"Year\",\"PaperId\"]).to_csv(\"output/Authors.csv\", mode='a', header=False, index=False)\n",
    "    pd.DataFrame(papers, columns=[\"Id\", \"Title\", \"EventType\", \"PdfName\", \"Abstract\", \"PaperText\"]).to_csv(\"output/Papers.csv\", mode='a', header=False, index=False)\n",
    "    pd.DataFrame(paper_authors, columns=[\"Id\", \"PaperId\", \"AuthorId\"]).to_csv(\"output/PaperAuthors.csv\", mode='a', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403 Papers Found\n",
      "411 Papers Found\n",
      "360 Papers Found\n",
      "368 Papers Found\n",
      "306 Papers Found\n",
      "292 Papers Found\n",
      "262 Papers Found\n",
      "250 Papers Found\n",
      "217 Papers Found\n",
      "204 Papers Found\n",
      "207 Papers Found\n",
      "207 Papers Found\n",
      "198 Papers Found\n",
      "207 Papers Found\n",
      "197 Papers Found\n",
      "152 Papers Found\n",
      "150 Papers Found\n",
      "151 Papers Found\n",
      "150 Papers Found\n",
      "152 Papers Found\n",
      "152 Papers Found\n",
      "140 Papers Found\n",
      "158 Papers Found\n",
      "127 Papers Found\n",
      "144 Papers Found\n",
      "143 Papers Found\n",
      "101 Papers Found\n",
      "94 Papers Found\n",
      "90 Papers Found\n"
     ]
    }
   ],
   "source": [
    "## Generating each year's total papers\n",
    "index_conf_year = 2015\n",
    "for i,url in enumerate(index_url_array):\n",
    "    nips_papers = []\n",
    "    index_url = index_url_array[i]\n",
    "    r = requests.get(index_url)\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    paper_links = [link for link in soup.find_all('a') if link[\"href\"][:7]==\"/paper/\"]\n",
    "    print(\"%d Papers Found\" % len(paper_links))\n",
    "    nips_papers.append([len(paper_links), index_conf_year])\n",
    "    index_conf_year -= 1\n",
    "    pd.DataFrame(nips_papers, columns=[\"Year\",\"Total_Paper\"]).to_csv(\"output/Years.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://papers.nips.cc/book/advances-in-neural-information-processing-systems-28-2015']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "base_url  = \"http://papers.nips.cc\"\n",
    "index_conf_year = 2015\n",
    "index_conf_number = 28\n",
    "index_url_array = []\n",
    "for i in range(1):\n",
    "    a = 'http://papers.nips.cc/book/advances-in-neural-information-processing-systems-%i-%i' %(index_conf_number,index_conf_year)\n",
    "    index_url_array.append(a)\n",
    "    index_conf_year -= 1\n",
    "    index_conf_number -= 1\n",
    "print index_url_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Again using above tweeked code to save just one year papers abstract\n",
    "for i,url in enumerate(index_url_array):\n",
    "    index_url = index_url_array[i]\n",
    "    r = requests.get(index_url)\n",
    "    ## BeautifulSoup package converts the HTML page into JSON format for convenient readability\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    paper_links = [link for link in soup.find_all('a') if link[\"href\"][:7]==\"/paper/\"]\n",
    "    ## Total Paper found every year\n",
    "    print(\"%d Papers Found\" % len(paper_links))\n",
    "    ## Initializing all lists()\n",
    "    nips_authors = list()\n",
    "    papers = list()\n",
    "    paper_authors = list()\n",
    "    authors_all = list()\n",
    "    email_author =  list()\n",
    "    email_id = list()\n",
    "    emails = []\n",
    "    tempEmail = []\n",
    "    event_type = []\n",
    "    \n",
    "    temp_path = os.path.join(\"output\", \"temp.txt\")\n",
    "    ## Iteration Loop for every year's each pdf or paper\n",
    "    for link in paper_links:\n",
    "        paper_title = link.contents[0]\n",
    "        info_link = base_url + link[\"href\"]\n",
    "        pdf_link = info_link + \".pdf\"\n",
    "        pdf_name = link[\"href\"][7:] + \".pdf\"\n",
    "        paper_id = re.findall(r\"^(\\d+)-\", pdf_name)[0]\n",
    "        pdf = requests.get(pdf_link)\n",
    "        folder_name = '%i' %index_conf_year\n",
    "        ## getting the whole webpage data in JSON format from Beautiful Soup.\n",
    "        paper_soup = BeautifulSoup(requests.get(info_link).content, \"lxml\")\n",
    "        ## Getting abstract from BeautifulSoup.\n",
    "        abstract = paper_soup.find('p', attrs={\"class\": \"abstract\"}).contents[0]\n",
    "        ## Getting authors from BeautifulSoup.\n",
    "        authors = [(re.findall(r\"-(\\d+)$\", author.contents[0][\"href\"])[0],\n",
    "                    author.contents[0].contents[0])\n",
    "                   for author in paper_soup.find_all('li', attrs={\"class\": \"author\"})]\n",
    "           \n",
    "        print(paper_title)\n",
    "\n",
    "        papers.append([paper_id, index_conf_year, paper_title, pdf_name, abstract])\n",
    "    index_conf_year -= 1\n",
    "    ## writing the appended lists to csv files with append true option.\n",
    "    pd.DataFrame(papers, columns=[\"Id\", \"Year\", \"Title\", \"PdfName\", \"Abstract\"]).to_csv(\"output/Papers_abstract.csv\", mode='a', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
